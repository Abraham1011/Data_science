---
title: 'Chat GPT y Minería de Texto: Analizando Noticias Económicas'
author: "Pedro Abraham Montoya Calzada"
date: "2023-12-19"
output: pdf_document
linkcolor: blue
header-includes:
   - \usepackage{inputenc}
   - \usepackage{fancyhdr}
   - \usepackage{babel}
   - \usepackage{graphicx}
   - \usepackage{float} 
---
\section{Resumen }

La minería de texto, también conocida como análisis de texto o procesamiento de lenguaje natural (NLP, por sus siglas en inglés), es un campo interdisciplinario que combina técnicas de la minería de datos, la lingüística computacional y la inteligencia artificial para extraer información valiosa y conocimiento a partir de grandes cantidades de datos de texto no estructurados.


Recientemente, con los modelos grandes de lenguaje (LLM) como: GPT, Llama 2, etc. Realizar estas tareas, que antes requerían un gran esfuerzo y conocimientos técnicos, se ha vuelto cada vez accesibles a cualquier persona o institución. 


Esta ocasión, he decido poner un ejemplo de cómo se pueden aprovechar estos modelos para extraer información sobre noticas de economía publicadas en el periódico [El Economista](https://www.eleconomista.com.mx/), utilizando los modelos GPT para extraer el sentimiento de las noticias publicadas e incrustaciones para realizar agrupación de noticias mediante aprendizaje no supervisado.


El proceso de extracción, transformación y carga de datos se realizo en Python, pueden consultar el script para la extracción automática de los datos en mi repositorio de GitHub.


\section{Análisis de sentimiento}


```{r,warning=FALSE, message=FALSE, echo=FALSE,fig.width=7, fig.height=4,fig.align='center'}
library(tidyverse)
library(ggplot2)

clean <- function(txt){
  txt_new <- tolower(txt)
  txt_new <- gsub("[0-9]", "", txt_new)
  txt_new <- gsub("[[:punct:]]", "", txt_new)
  txt_new <- gsub("\\s+", " ", txt_new)
  return(txt_new)
}

notas <- read.csv('Economic_news.csv')
sentimiento <- read.csv('Sentiment.csv')

df_sent <- notas %>% 
  inner_join(sentimiento)

df_sent$Date <- as.Date(df_sent$Date)

df_sent$txt <- clean(df_sent$Text)

ggplot(data = df_sent) + 
  geom_bar(aes(x = Sentiment, fill = as.factor(Sentiment))) + 
  theme_bw() + 
  theme(plot.title=element_text(hjust=0.5,size=17)) + 
  scale_fill_manual(values = c("#FF5555","lightblue","#76DF66")) + 
  labs(x = "Sentimiento", y = "Frecuencia",
       fill = "", title = "Sentimiento de las noticias")


```



```{r,warning=FALSE, message=FALSE, echo=FALSE,fig.width=8, fig.height=4,fig.align='center'}
library(tidytext)
library(tm)

n <- 10

stop_words <- data.frame(word = c("la","los","de","a",
                                  "que","del","las","con",
                                  "para","por","una","más",
                                  "como","desde","este",
                                  "sobre","año","entre"))


color <- c("#FF5555","lightblue","#76DF66")

g <- list()
c = 1
for (i in c("Negativo","Neutro","Positivo")){
  df_sentimiento <- df_sent %>% 
  filter(Sentiment == i)
  corpus <- Corpus(VectorSource(df_sentimiento$txt))
  tdm <- TermDocumentMatrix(corpus)
  word <- as.matrix(tdm)
  word_freq <- rowSums(word)
  word_freq <- data.frame(word = names(word_freq),
                             freq = word_freq)
  word_freq <- word_freq %>% 
    anti_join(stop_words)
  word_freq <- word_freq %>% 
    arrange(desc(freq))
  
  g[[c]] <- ggplot(word_freq[1:n,],
         aes(y = reorder(word, freq),x = freq))+
        geom_col(fill = color[c]) + 
    theme_bw() + 
    theme(plot.title=element_text(hjust=0.5,size=10)) + 
    labs(y = "Palabras", x = "Frecuencia",
       fill = "", title = i)
  c = c + 1
}

library(patchwork)

g[[1]] + g[[2]] + g[[3]] + 
  plot_layout(ncol = 3, nrow = 1)


```


\section{Agrupamiento de noticias}


```{r,warning=FALSE, message=FALSE, echo=FALSE,include=FALSE}
library(reticulate)

embedding <- read.csv('Embedding.csv')

df_emb <- embedding[, !colnames(embedding) %in% "Id"]

K_means <- import("sklearn.cluster")$KMeans
k_means <- K_means(n_clusters = as.integer(3))
k_means$fit(df_emb)


PCA <- import("sklearn.decomposition")$PCA(n_components = as.integer(2))

pca <- PCA$fit_transform(df_emb)

grupos <- k_means$labels_ + 1

```


```{r,warning=FALSE, message=FALSE, echo=FALSE,fig.width=7, fig.height=4,fig.align='center'}
ggplot() + 
  geom_point(aes(x = pca[,1], y = pca[,2],
                 color = as.factor(grupos)),
             size = 3) + 
      theme_bw() + 
    theme(plot.title=element_text(hjust=0.5,size=16)) + 
    labs(x = "PCA 1", y = "PCA 2",
       color = "Grupo", title = "Visualización en 2D de los 3 grupos formados")

```

\newpage

\subsection{Nube de palabras para el grupo 1}

```{r,warning=FALSE, message=FALSE, echo=FALSE,include=FALSE}
library(wordcloud2)

df_grupos <- cbind(notas, grupos)

df_grupos$txt <- clean(df_grupos$Text)

df_grupo1 <- df_grupos %>%
  filter(grupos == 1)


n <- 50

stop_words <- data.frame(word = c("la","los","de","a",
                                  "que","del","las","con",
                                  "para","por","una","más",
                                  "como","desde","este",
                                  "sobre","año","entre",
                                  "y","se","en","al","lo",
                                  "un","ha","si","ya","es",
                                  "su","o","sin","esto",
                                  "han","no","le","son",
                                  "pero","esta"))

token <- df_grupo1 %>% 
  unnest_tokens(word, txt,token = "ngrams", n = 1) %>%
  anti_join(stop_words)

word <- token %>% count(word, sort = TRUE)
unigrama <- wordcloud2(word[1:n,][-1,],color = rainbow(30),
           minRotation = 0,maxRotation = 0,
           fontFamily ="Calibri" )


library(webshot)
library(htmlwidgets)
saveWidget(unigrama,"unigrama.html",selfcontained = FALSE)

webshot("unigrama.html","unigrama.png",delay = 5, 
        vwidth = 1000, 
        vheight = 1000)

```


\begin{figure}[H]
\centering
\includegraphics{unigrama}
\end{figure}


\newpage

\subsection{Nube de palabras para el grupo 2}

```{r,warning=FALSE, message=FALSE, echo=FALSE,include=FALSE}
library(wordcloud2)

df_grupos <- cbind(notas, grupos)

df_grupos$txt <- clean(df_grupos$Text)

df_grupo1 <- df_grupos %>%
  filter(grupos == 2)


n <- 50

stop_words <- data.frame(word = c("la","los","de","a",
                                  "que","del","las","con",
                                  "para","por","una","más",
                                  "como","desde","este",
                                  "sobre","año","entre",
                                  "y","se","en","al","lo",
                                  "un","ha","si","ya","es",
                                  "su","o","sin","esto",
                                  "han","no","le","son",
                                  "pero","esta"))

token <- df_grupo1 %>% 
  unnest_tokens(word, txt,token = "ngrams", n = 1) %>%
  anti_join(stop_words)

word <- token %>% count(word, sort = TRUE)
unigrama <- wordcloud2(word[1:n,][-1,],color = rainbow(30),
           minRotation = 0,maxRotation = 0,
           fontFamily ="Calibri" )


library(webshot)
library(htmlwidgets)
saveWidget(unigrama,"unigrama2.html",selfcontained = FALSE)

webshot("unigrama2.html","unigrama2.png",delay = 5, 
        vwidth = 1000, 
        vheight = 1000)

```


\begin{figure}[H]
\centering
\includegraphics{unigrama2}
\end{figure}


\newpage

\subsection{Nube de palabras para el grupo 3}

```{r,warning=FALSE, message=FALSE, echo=FALSE,include=FALSE}
library(wordcloud2)

df_grupos <- cbind(notas, grupos)

df_grupos$txt <- clean(df_grupos$Text)

df_grupo1 <- df_grupos %>%
  filter(grupos == 3)


n <- 50

stop_words <- data.frame(word = c("la","los","de","a",
                                  "que","del","las","con",
                                  "para","por","una","más",
                                  "como","desde","este",
                                  "sobre","año","entre",
                                  "y","se","en","al","lo",
                                  "un","ha","si","ya","es",
                                  "su","o","sin","esto",
                                  "han","no","le","son",
                                  "pero","esta"))

token <- df_grupo1 %>% 
  unnest_tokens(word, txt,token = "ngrams", n = 1) %>%
  anti_join(stop_words)

word <- token %>% count(word, sort = TRUE)
unigrama <- wordcloud2(word[1:n,][-1,],color = rainbow(30),
           minRotation = 0,maxRotation = 0,
           fontFamily ="Calibri" )


library(webshot)
library(htmlwidgets)
saveWidget(unigrama,"unigrama3.html",selfcontained = FALSE)

webshot("unigrama3.html","unigrama3.png",delay = 5, 
        vwidth = 1000, 
        vheight = 1000)

```


\begin{figure}[H]
\centering
\includegraphics{unigrama3}
\end{figure}



